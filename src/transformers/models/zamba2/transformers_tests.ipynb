{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fcda9ded7e4ec08f5f7ebff304bea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f532f676e2d547beaccbf9b954ec025f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75566b8caa2496b8202ec25ad39820c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "base_model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    #quantization_config=bnb_config,\n",
    "    #device_map=\"auto\",\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89659a5b7bc4a99894ede73294f4d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9942d65af44bdab0795767cb61d198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5737ffa1f21e4df7b45fe97b9c15dc49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b2a67b10284eca83ac26d6dddd9187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.9 s, sys: 0 ns, total: 1.9 s\n",
      "Wall time: 124 ms\n",
      "tensor([[[ -4.6822,   0.9866,   4.5126,  ...,  -5.2010,  -2.1646,  -4.2286],\n",
      "         [-10.1421, -10.1378,   5.6231,  ...,  -6.1040, -11.0720,  -4.8641],\n",
      "         [ -9.6640,  -9.2623,   4.9745,  ...,  -7.9037, -11.2208,  -3.2938],\n",
      "         [ -8.7681,  -8.3673,   8.2792,  ...,  -5.3513,  -8.6309,  -3.2394]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "text = \"Sample input.\"\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "%time output = model(input_ids)\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "from transformers.models.zamba2 import Zamba2Config, Zamba2Model\n",
    "config = Zamba2Config(# hidden_size=256,\n",
    "                    use_cache=False,\n",
    "                    use_mamba_kernels=True)\n",
    "\n",
    "model = Zamba2Model(config)\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 5120])\n",
      "<function MLP.__init__.<locals>.glu at 0x7fbbf42c8700>\n",
      "tensor([[[-0.2819,  0.8015,  0.4129,  ...,  0.4797,  1.4796,  0.8483],\n",
      "         [-0.2460, -0.1759,  1.7739,  ..., -1.7110,  1.0436,  0.6792]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 1, 5120])\n",
      "<function MLP.__init__.<locals>.glu at 0x7fba696c60e0>\n",
      "tensor([[[-1.0465, -2.1433,  2.8452,  ..., -0.7424, -0.3116, -0.0337],\n",
      "         [ 0.4994, -2.6086,  0.6702,  ..., -2.1001,  0.5389, -1.2339]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 1, 5120])\n",
      "<function MLP.__init__.<locals>.glu at 0x7fbbf42c8700>\n",
      "tensor([[[ 1.2252,  0.2309,  1.2017,  ...,  0.8811,  0.8888, -0.8302],\n",
      "         [ 0.0926,  0.8126, -0.1900,  ...,  0.5705,  1.3784, -1.2281]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 1, 5120])\n",
      "<function MLP.__init__.<locals>.glu at 0x7fba696c60e0>\n",
      "tensor([[[-1.3620, -1.1314,  1.5622,  ..., -2.2054,  0.3344, -1.4363],\n",
      "         [-0.3542, -1.3665,  2.4731,  ...,  0.0512,  0.1473, -2.0355]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 1, 5120])\n",
      "<function MLP.__init__.<locals>.glu at 0x7fbbf42c8700>\n",
      "tensor([[[ 1.4030, -0.6196,  0.4053,  ...,  0.7696,  0.6787, -0.3625],\n",
      "         [-0.9614, -0.2090,  0.7659,  ...,  0.2274,  0.7983,  0.1795]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 1, 5120])\n",
      "<function MLP.__init__.<locals>.glu at 0x7fba696c60e0>\n",
      "tensor([[[-1.8156, -1.0112,  1.7077,  ..., -1.4515,  0.4265, -1.3470],\n",
      "         [-1.3153, -0.8937,  1.2847,  ..., -0.2009, -0.3130, -2.2120]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 1, 5120])\n",
      "<function MLP.__init__.<locals>.glu at 0x7fbbf42c8700>\n",
      "tensor([[[ 1.4890, -0.6031,  0.1128,  ...,  1.6625,  0.4016, -0.0189],\n",
      "         [ 0.0265, -0.1550, -0.7009,  ...,  1.7750,  1.0250,  1.0035]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 1, 5120])\n",
      "<function MLP.__init__.<locals>.glu at 0x7fba696c60e0>\n",
      "tensor([[[-0.7267, -0.2827,  1.8515,  ..., -0.6965,  1.3685, -0.8493],\n",
      "         [-0.3524, -1.3992,  1.5158,  ..., -0.9949,  0.6039, -1.8277]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 1, 5120])\n",
      "<function MLP.__init__.<locals>.glu at 0x7fbbf42c8700>\n",
      "tensor([[[ 0.8211, -0.8118,  0.2696,  ...,  2.4066,  0.7809,  0.1456],\n",
      "         [ 0.0828,  0.2120, -0.9254,  ...,  2.1388,  0.9367, -0.3394]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPast(last_hidden_state=tensor([[[-0.9624, -2.4215, -0.4542,  ...,  0.4463,  2.5638,  0.6742],\n",
       "         [ 0.8970,  0.5811, -1.5586,  ..., -1.2077, -0.4730,  0.8435]]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "inp = torch.tensor([[1, 2]]).cuda()\n",
    "model(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint to .bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "\n",
    "import torch\n",
    "# from transformers import ZambaForCausalLM, ZambaConfig, AutoTokenizer\n",
    "# from transformers.models.jamba.modeling_jamba import HybridMambaAttentionDynamicCache\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from megatron import checkpointing\n",
    "import sys, torch, os\n",
    "from megatron.initialize import initialize_megatron\n",
    "\n",
    "# CONFIG_PATH = \"/workspace/mamba_memory_block_7B_annealing_nochat_noTP.sh\"\n",
    "# CHECKPOINT_PATH = \"/checkpoints/mamba_memory_block_7B_annealing_nochat_reformatted_openOrca_noTP\"\n",
    "# ITERATION = 25156\n",
    "\n",
    "CONFIG_PATH = \"/workspace/Mamba-MoE/examples/mamba_memory_block/zamba_3B_annealing_final.sh\"\n",
    "CHECKPOINT_PATH = \"/checkpoints/zamba_3B_attempt_2\"\n",
    "ITERATION = 462070\n",
    "\n",
    "\n",
    "def extract_keyword_args(filestr, keyword):\n",
    "    gpt_split = filestr.split(keyword)\n",
    "    if len(gpt_split) <=1:\n",
    "        raise ValueError(\"Config provided does not have a GPT_ARGS variable provided\")\n",
    "    arg_splits = gpt_split[1].split(\"\\\"\")\n",
    "    gpt_args = arg_splits[1]\n",
    "    gpt_args = gpt_args.replace(\"\\n\",\"\").replace(\"\\\\\",\"\").replace(\"\\t\",\"\")\n",
    "    gpt_args = ' '.join(gpt_args.split())\n",
    "    return gpt_args.strip().split(\" \")\n",
    "\n",
    "\n",
    "def extract_data_paths(filestr, CHECKPOINT_PATH):\n",
    "    tokenizer_type = filestr.split(\"TOKENIZER_TYPE=\")[1].split(\"\\n\")[0]\n",
    "    tokenizer_model = filestr.split(\"TOKENIZER_MODEL=\")[1].split(\"\\n\")[0]\n",
    "\n",
    "    data_path = filestr.split(\"DATA_PATH=\")[1].split(\"\\n\")[0]\n",
    "    return [\"--data-path\", data_path, \"--tokenizer-type\", tokenizer_type, \"--hf_autotokenizer_model\", tokenizer_model, \"--load\", CHECKPOINT_PATH]\n",
    "\n",
    "def parse_config_file_update_argv(CONFIG_PATH, CHECKPOINT_PATH):\n",
    "    with open(CONFIG_PATH,\"r\") as f:\n",
    "        filestr = f.read()\n",
    "    \n",
    "    sys.argv = [\"\"]\n",
    "    sys.argv += extract_keyword_args(filestr, \"GPT_ARGS\")\n",
    "    sys.argv += extract_data_paths(filestr, CHECKPOINT_PATH)\n",
    "\n",
    "def init_megatron():\n",
    "    initialize_megatron(\n",
    "        args_defaults={\n",
    "            'tokenizer_type': 'HFAutoTokenizer',\n",
    "            'hf_autotokenizer_model': 'EleutherAI/gpt-neox-20b',\n",
    "            'no_load_rng': True,\n",
    "            'no_load_optim': True\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_MAX_CONNECTIONS\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"6000\"\n",
    "parse_config_file_update_argv(CONFIG_PATH, CHECKPOINT_PATH)\n",
    "init_megatron()\n",
    "\n",
    "# name = checkpointing.get_checkpoint_name(\n",
    "#     CHECKPOINT_PATH, \n",
    "#     iteration=ITERATION,\n",
    "#     #tensor_rank=i,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /checkpoints/zamba_3B_attempt_2/final_anneal/mp_rank_00/model_optim_rng.pt\n"
     ]
    }
   ],
   "source": [
    "name = '/checkpoints/zamba_3B_attempt_2/final_anneal/mp_rank_00/model_optim_rng.pt'\n",
    "print(f\"Loading {name}\")\n",
    "#state_dicts.append(torch.load(name, map_location='cpu'))\n",
    "state_dict = torch.load(name, map_location='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save state dictionary\n",
    "\n",
    "import pickle\n",
    "dict_1 = state_dict['model']\n",
    "with open('/workspace/transformers_zamba2_dev/src/transformers/models/zamba2/state_dict.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(dict_1, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state dictionary\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "import pickle\n",
    "with open('/workspace/transformers_zamba2_dev/src/transformers/models/zamba2/state_dict.pkl', 'rb') as pickle_file:\n",
    "    dict_1 = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "from transformers.models.zamba2 import Zamba2Config, Zamba2ForCausalLM\n",
    "config = Zamba2Config()\n",
    "model = Zamba2ForCausalLM(config).cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.word_embeddings.weight torch.Size([32000, 2560])\n",
      "embedding.position_embeddings.weight torch.Size([4096, 2560])\n",
      "decoder.block_map.6.weight torch.Size([2560, 2560])\n",
      "decoder.block_map.12.weight torch.Size([2560, 2560])\n",
      "decoder.block_map.18.weight torch.Size([2560, 2560])\n",
      "decoder.block_map.24.weight torch.Size([2560, 2560])\n",
      "decoder.block_map.30.weight torch.Size([2560, 2560])\n",
      "decoder.block_map.36.weight torch.Size([2560, 2560])\n",
      "decoder.block_map.42.weight torch.Size([2560, 2560])\n",
      "decoder.block_map.47.weight torch.Size([2560, 2560])\n",
      "decoder.block_map.51.weight torch.Size([2560, 2560])\n",
      "decoder.final_layernorm.weight torch.Size([2560])\n",
      "decoder.buffer_params_1.weight torch.Size([1, 48])\n",
      "\n",
      "model.embed_tokens.weight torch.Size([32000, 2560])\n",
      "model.linear_layers.0.weight torch.Size([2560, 2560])\n",
      "model.linear_layers.1.weight torch.Size([2560, 2560])\n",
      "model.linear_layers.2.weight torch.Size([2560, 2560])\n",
      "model.linear_layers.3.weight torch.Size([2560, 2560])\n",
      "model.linear_layers.4.weight torch.Size([2560, 2560])\n",
      "model.linear_layers.5.weight torch.Size([2560, 2560])\n",
      "model.linear_layers.6.weight torch.Size([2560, 2560])\n",
      "model.linear_layers.7.weight torch.Size([2560, 2560])\n",
      "model.linear_layers.8.weight torch.Size([2560, 2560])\n",
      "model.final_layernorm.weight torch.Size([2560])\n"
     ]
    }
   ],
   "source": [
    "for key, value in dict_1.items():\n",
    "    if 'decoder.layers.' not in key and 'decoder.blocks' not in key:\n",
    "        print(key , value.shape)\n",
    "print()\n",
    "for n, p in model.named_parameters():\n",
    "    if 'mamba_layers.' not in n and 'blocks.' not in n:\n",
    "        print(n, p.shape)\n",
    "\n",
    "count = 0\n",
    "for n, p in model.named_parameters():\n",
    "    if 'mamba_layers.' not in n and 'blocks.' not in n:\n",
    "        if 'embed_tokens' in n:\n",
    "            assert p.data.shape == dict_1['embedding.word_embeddings.weight'].shape\n",
    "            p.data = dict_1['embedding.word_embeddings.weight'].clone().detach()\n",
    "        if 'final_layernorm' in n:\n",
    "            assert p.data.shape == dict_1['decoder.final_layernorm.weight'].shape\n",
    "            p.data = dict_1['decoder.final_layernorm.weight'].clone().detach()\n",
    "        if 'linear_layers' in n:\n",
    "            num = int(n[20])\n",
    "            num2num = [6, 12, 18, 24, 30, 36, 42, 47, 51]\n",
    "            name = 'decoder.block_map.'+str(num2num[num])+'.weight'\n",
    "            assert p.data.shape == dict_1[name].shape\n",
    "            p.data = dict_1[name].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.layers.0.mixer.dt_bias torch.Size([80])\n",
      "decoder.layers.0.mixer.A_log torch.Size([80])\n",
      "decoder.layers.0.mixer.D torch.Size([80])\n",
      "decoder.layers.0.mixer.in_proj.0.weight torch.Size([10448, 2560])\n",
      "decoder.layers.0.mixer.conv1d.weight torch.Size([5248, 1, 4])\n",
      "decoder.layers.0.mixer.conv1d.bias torch.Size([5248])\n",
      "decoder.layers.0.mixer.norm.weight torch.Size([5120])\n",
      "decoder.layers.0.mixer.out_proj.weight torch.Size([2560, 5120])\n",
      "decoder.layers.0.norm.weight torch.Size([2560])\n",
      "\n",
      "model.mamba_layers.0.mamba.dt_bias torch.Size([80])\n",
      "model.mamba_layers.0.mamba.A_log torch.Size([80])\n",
      "model.mamba_layers.0.mamba.D torch.Size([80])\n",
      "model.mamba_layers.0.mamba.in_proj.0.weight torch.Size([10448, 2560])\n",
      "model.mamba_layers.0.mamba.conv1d.weight torch.Size([5248, 1, 4])\n",
      "model.mamba_layers.0.mamba.conv1d.bias torch.Size([5248])\n",
      "model.mamba_layers.0.mamba.norm.weight torch.Size([5120])\n",
      "model.mamba_layers.0.mamba.out_proj.weight torch.Size([2560, 5120])\n",
      "model.mamba_layers.0.input_layernorm.weight torch.Size([2560])\n"
     ]
    }
   ],
   "source": [
    "def replace(input, to_replace, replace_with):\n",
    "    idx = input.find(to_replace)\n",
    "    l = len(to_replace)\n",
    "    return input[:idx] + replace_with + input[idx+l:]\n",
    "\n",
    "for key, value in dict_1.items():        \n",
    "    if 'decoder.layers.0' in key:\n",
    "        print(key , value.shape)\n",
    "print()\n",
    "for n, p in model.named_parameters():\n",
    "    if 'mamba_layers.0' in n:\n",
    "        print(n, p.shape)\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if 'mamba_layers.' in n:\n",
    "        name = 'decoder.layers.' + n[19:]\n",
    "        if 'input_layernorm' in name:\n",
    "            name = replace(name, 'input_layernorm', 'norm')\n",
    "        else:\n",
    "            name = replace(name, 'mamba', 'mixer')\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.blocks.0.sa.mixer.linear_proj.weight torch.Size([2560, 5120])\n",
      "decoder.blocks.0.sa.mixer.linear_qkv.weight torch.Size([15360, 5120])\n",
      "decoder.blocks.0.sa.norm.weight torch.Size([5120])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1.weight torch.Size([20480, 2560])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc2.weight torch.Size([2560, 10240])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.0.weight torch.Size([128, 2560])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.1.weight torch.Size([128, 2560])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.2.weight torch.Size([128, 2560])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.3.weight torch.Size([128, 2560])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.4.weight torch.Size([128, 2560])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.5.weight torch.Size([128, 2560])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.6.weight torch.Size([128, 2560])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.7.weight torch.Size([128, 2560])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.8.weight torch.Size([128, 2560])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.0.weight torch.Size([20480, 128])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.1.weight torch.Size([20480, 128])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.2.weight torch.Size([20480, 128])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.3.weight torch.Size([20480, 128])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.4.weight torch.Size([20480, 128])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.5.weight torch.Size([20480, 128])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.6.weight torch.Size([20480, 128])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.7.weight torch.Size([20480, 128])\n",
      "decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.8.weight torch.Size([20480, 128])\n",
      "decoder.blocks.0.mlp.norm.weight torch.Size([2560])\n",
      "\n",
      "model.blocks.0.self_attn.linear_qkv.weight torch.Size([15360, 5120])\n",
      "model.blocks.0.self_attn.linear_proj.weight torch.Size([2560, 5120])\n",
      "model.blocks.0.feed_forward.linear_fc1.weight torch.Size([20480, 2560])\n",
      "model.blocks.0.feed_forward.linear_fc2.weight torch.Size([2560, 10240])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_A_list.0.weight torch.Size([128, 2560])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_A_list.1.weight torch.Size([128, 2560])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_A_list.2.weight torch.Size([128, 2560])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_A_list.3.weight torch.Size([128, 2560])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_A_list.4.weight torch.Size([128, 2560])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_A_list.5.weight torch.Size([128, 2560])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_A_list.6.weight torch.Size([128, 2560])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_A_list.7.weight torch.Size([128, 2560])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_A_list.8.weight torch.Size([128, 2560])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.0.weight torch.Size([20480, 128])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.1.weight torch.Size([20480, 128])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.2.weight torch.Size([20480, 128])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.3.weight torch.Size([20480, 128])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.4.weight torch.Size([20480, 128])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.5.weight torch.Size([20480, 128])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.6.weight torch.Size([20480, 128])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.7.weight torch.Size([20480, 128])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.8.weight torch.Size([20480, 128])\n",
      "model.blocks.0.input_layernorm.weight torch.Size([5120])\n",
      "model.blocks.0.pre_ff_layernorm.weight torch.Size([2560])\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.0.weight decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.0.weight\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.1.weight decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.1.weight\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.2.weight decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.2.weight\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.3.weight decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.3.weight\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.4.weight decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.4.weight\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.5.weight decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.5.weight\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.6.weight decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.6.weight\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.7.weight decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.7.weight\n",
      "model.blocks.0.feed_forward.linear_fc1_lora_B_list.8.weight decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.8.weight\n",
      "model.blocks.1.feed_forward.linear_fc1_lora_A_list.0.weight decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.0.weight 0\n",
      "model.blocks.1.feed_forward.linear_fc1_lora_A_list.1.weight decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.1.weight 1\n",
      "model.blocks.1.feed_forward.linear_fc1_lora_A_list.2.weight decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.2.weight 2\n",
      "model.blocks.1.feed_forward.linear_fc1_lora_A_list.3.weight decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.3.weight 3\n",
      "model.blocks.1.feed_forward.linear_fc1_lora_A_list.4.weight decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.4.weight 4\n",
      "model.blocks.1.feed_forward.linear_fc1_lora_A_list.5.weight decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.5.weight 5\n",
      "model.blocks.1.feed_forward.linear_fc1_lora_A_list.6.weight decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.6.weight 6\n",
      "model.blocks.1.feed_forward.linear_fc1_lora_A_list.7.weight decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.7.weight 7\n",
      "model.blocks.1.feed_forward.linear_fc1_lora_A_list.8.weight decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.8.weight 8\n"
     ]
    }
   ],
   "source": [
    "for key, value in dict_1.items():\n",
    "    if 'decoder.blocks.0' in key and 'extra_state' not in key:\n",
    "        print(key, value.shape)\n",
    "print()\n",
    "for n, p in model.named_parameters():\n",
    "    if 'blocks.0' in n:\n",
    "        print(n, p.shape)\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if 'blocks.0.self_attn.linear_qkv.weight' in n:\n",
    "        name = 'decoder.blocks.0.sa.mixer.linear_qkv.weight'\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.0.self_attn.linear_proj.weight' in n:\n",
    "        name = 'decoder.blocks.0.sa.mixer.linear_proj.weight'\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.1.self_attn.linear_qkv.weight' in n:\n",
    "        name = 'decoder.blocks.1.sa.mixer.linear_qkv.weight'\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.1.self_attn.linear_proj.weight' in n:\n",
    "        name = 'decoder.blocks.1.sa.mixer.linear_proj.weight'\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "\n",
    "    if 'blocks.0.feed_forward.linear_fc1.weight' in n:\n",
    "        name = 'decoder.blocks.0.mlp.mixer.linear_fc1.weight'\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.0.feed_forward.linear_fc2.weight' in n:\n",
    "        name = 'decoder.blocks.0.mlp.mixer.linear_fc2.weight'\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.1.feed_forward.linear_fc1.weight' in n:\n",
    "        name = 'decoder.blocks.1.mlp.mixer.linear_fc1.weight'\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.1.feed_forward.linear_fc2.weight' in n:\n",
    "        name = 'decoder.blocks.1.mlp.mixer.linear_fc2.weight'\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "        \n",
    "    if 'blocks.0.input_layernorm.weight' in n:\n",
    "        name = 'decoder.blocks.0.sa.norm.weight'\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.1.input_layernorm.weight' in n:\n",
    "        name = 'decoder.blocks.1.sa.norm.weight'\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.0.pre_ff_layernorm.weight' in n:\n",
    "        name = 'decoder.blocks.0.mlp.norm.weight'\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.1.pre_ff_layernorm.weight' in n:\n",
    "        name = 'decoder.blocks.1.mlp.norm.weight'\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.0.feed_forward.linear_fc1_lora_A' in n:\n",
    "        num = replace(n, 'model.blocks.0.feed_forward.linear_fc1_lora_', '')[7]\n",
    "        name = 'decoder.blocks.0.mlp.mixer.linear_fc1_lora' + replace('_A_list.0.weight', '0', num)\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.0.feed_forward.linear_fc1_lora_B' in n:\n",
    "        num = replace(n, 'model.blocks.0.feed_forward.linear_fc1_lora_', '')[7]\n",
    "        name = 'decoder.blocks.0.mlp.mixer.linear_fc1_lora' + replace('_B_list.0.weight', '0', num)\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.1.feed_forward.linear_fc1_lora_A' in n:\n",
    "        num = replace(n, 'model.blocks.1.feed_forward.linear_fc1_lora_', '')[7]\n",
    "        name = 'decoder.blocks.1.mlp.mixer.linear_fc1_lora' + replace('_A_list.0.weight', '0', num)\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()\n",
    "    if 'blocks.1.feed_forward.linear_fc1_lora_B' in n:\n",
    "        num = replace(n, 'model.blocks.1.feed_forward.linear_fc1_lora_', '')[7]\n",
    "        name = 'decoder.blocks.1.mlp.mixer.linear_fc1_lora' + replace('_B_list.0.weight', '0', num)\n",
    "        assert p.data.shape == dict_1[name].shape\n",
    "        p.data = dict_1[name].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Today is the first time that\n",
      "<function MLP.__init__.<locals>.glu at 0x7f3307637b50>\n",
      "tensor([[[-0.3457,  0.0161,  0.1758,  ..., -0.2480, -0.1328, -0.4062],\n",
      "         [-0.3535,  0.0093,  0.0625,  ..., -0.0267, -0.0449, -0.1768],\n",
      "         [-0.1167, -0.1162,  0.2227,  ..., -0.0854, -0.0908, -0.0547],\n",
      "         ...,\n",
      "         [ 0.0415, -0.3594, -0.2305,  ...,  0.2520, -0.0361, -0.0049],\n",
      "         [-0.0762, -0.2373, -0.3398,  ...,  0.1445,  0.2275, -0.0957],\n",
      "         [ 0.1162, -0.1562, -0.2852,  ...,  0.2031, -0.0752, -0.0024]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>)\n",
      "<function MLP.__init__.<locals>.glu at 0x7f330687ecb0>\n",
      "tensor([[[-0.1514, -0.1504, -0.5312,  ...,  0.5117,  0.2129,  1.0781],\n",
      "         [ 0.0017, -0.4336, -0.4023,  ...,  0.0776, -0.1865, -0.1187],\n",
      "         [-0.0659, -0.2539, -0.3262,  ...,  0.3184,  0.0674, -0.0015],\n",
      "         ...,\n",
      "         [-0.4004, -0.2871, -0.3223,  ...,  0.3105, -0.1216,  0.2812],\n",
      "         [-0.2656, -0.4922, -0.0581,  ...,  0.2695, -0.1523,  0.1099],\n",
      "         [-0.2334, -0.3203,  0.0044,  ...,  0.0762, -0.1338,  0.3242]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>)\n",
      "<function MLP.__init__.<locals>.glu at 0x7f3307637b50>\n",
      "tensor([[[-0.1299, -0.1719,  0.8477,  ..., -1.1875, -0.4531, -0.4727],\n",
      "         [-0.5039, -0.4512,  0.1367,  ...,  0.4258, -0.0850, -0.0581],\n",
      "         [-0.2256, -0.3770,  0.4570,  ...,  0.4531, -0.1113, -0.1357],\n",
      "         ...,\n",
      "         [ 0.4863, -0.6797, -0.3242,  ...,  0.4453,  0.0090, -0.2676],\n",
      "         [ 0.0474, -0.6523, -0.6992,  ..., -0.1621, -0.0215, -0.5664],\n",
      "         [ 0.2451, -0.8203, -0.0938,  ...,  0.1187, -0.2363, -0.3184]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>)\n",
      "<function MLP.__init__.<locals>.glu at 0x7f330687ecb0>\n",
      "tensor([[[-0.9492, -0.8672, -0.5391,  ...,  0.3398,  0.4609,  2.0938],\n",
      "         [-0.7305, -0.3711, -0.3672,  ...,  0.1504, -0.3301,  0.1040],\n",
      "         [-0.2871, -0.0259, -0.3477,  ...,  0.2197,  0.0337,  0.6328],\n",
      "         ...,\n",
      "         [-0.7148, -0.3242, -0.3594,  ...,  0.6484,  0.0273,  1.0625],\n",
      "         [-0.7695, -0.7812,  0.0244,  ...,  0.5039, -0.0042,  0.7695],\n",
      "         [-0.5312, -0.1069,  0.0684,  ...,  0.4492,  0.0112,  0.5586]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>)\n",
      "<function MLP.__init__.<locals>.glu at 0x7f3307637b50>\n",
      "tensor([[[ 0.2344, -0.9805,  0.4844,  ..., -0.7656, -0.6367, -1.0859],\n",
      "         [-0.2852, -0.7969,  0.2793,  ..., -0.1357, -0.4414, -0.6719],\n",
      "         [ 0.4043, -0.7891,  0.4648,  ...,  0.2891, -0.2090, -0.7266],\n",
      "         ...,\n",
      "         [ 0.5586, -0.3633,  0.3633,  ...,  0.7578, -0.3242, -0.2773],\n",
      "         [ 0.1895, -0.8750,  0.1060,  ..., -0.3242, -0.2207, -1.1094],\n",
      "         [ 0.3867, -1.3750,  0.3984,  ..., -0.9688, -0.3066, -0.8594]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>)\n",
      "<function MLP.__init__.<locals>.glu at 0x7f330687ecb0>\n",
      "tensor([[[-2.0938, -0.0415, -2.6875,  ...,  0.5312,  0.5391,  1.4766],\n",
      "         [-1.3594, -0.1367, -1.4922,  ...,  0.5234, -0.4648,  1.0469],\n",
      "         [-1.6641,  0.8516, -1.5547,  ...,  0.2695,  0.0042,  0.9961],\n",
      "         ...,\n",
      "         [-2.2188,  0.7148, -1.8906,  ...,  0.2949, -0.1846,  1.8594],\n",
      "         [-1.6797,  0.4551, -0.6328,  ...,  0.3945,  0.3262,  1.9688],\n",
      "         [-1.6875,  0.8516, -0.6680,  ...,  0.3281,  0.1162,  1.0156]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>)\n",
      "<function MLP.__init__.<locals>.glu at 0x7f3307637b50>\n",
      "tensor([[[-0.9922, -2.0156, -1.7031,  ...,  1.6250, -0.5078,  0.9258],\n",
      "         [-1.0391, -2.3125, -1.5078,  ...,  0.6484,  2.2031, -1.5000],\n",
      "         [-0.2363, -1.9531, -1.4453,  ...,  1.0625,  0.4375,  0.4531],\n",
      "         ...,\n",
      "         [-0.3164, -1.0938, -0.6602,  ...,  0.0176,  0.5547,  1.0938],\n",
      "         [-0.8594, -1.8125, -1.5703,  ..., -0.1680,  0.4668, -1.3672],\n",
      "         [-0.0684, -1.8047, -0.9414,  ..., -0.0117,  0.6641, -0.3770]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>)\n",
      "<function MLP.__init__.<locals>.glu at 0x7f330687ecb0>\n",
      "tensor([[[-5.3750e+00, -6.0156e-01,  3.9062e-03,  ...,  1.7188e+00,\n",
      "          -1.8125e+00,  3.0312e+00],\n",
      "         [-3.5938e+00,  3.3203e-02, -2.7734e-01,  ...,  1.4453e+00,\n",
      "          -1.3594e+00, -9.5703e-02],\n",
      "         [-4.1875e+00,  2.1250e+00, -1.3906e+00,  ...,  2.3281e+00,\n",
      "          -6.3281e-01,  2.2188e+00],\n",
      "         ...,\n",
      "         [-4.4688e+00,  1.2812e+00, -3.7812e+00,  ...,  3.6875e+00,\n",
      "          -2.5781e-01,  3.0000e+00],\n",
      "         [-4.3125e+00,  9.9219e-01, -1.7656e+00,  ...,  3.4062e+00,\n",
      "          -9.6094e-01,  2.5938e+00],\n",
      "         [-4.6562e+00,  2.3906e+00, -2.0312e+00,  ...,  3.1719e+00,\n",
      "          -9.1797e-01,  2.4375e+00]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<AddBackward0>)\n",
      "<function MLP.__init__.<locals>.glu at 0x7f3307637b50>\n",
      "tensor([[[-2.5938, -1.5156, -2.8594,  ..., -1.5703,  3.0000,  0.0781],\n",
      "         [-0.6914, -1.8906, -1.5547,  ...,  0.1895,  0.9961,  1.8750],\n",
      "         [-1.5000, -3.8438, -1.9922,  ..., -0.3711,  1.7031,  2.2344],\n",
      "         ...,\n",
      "         [-1.6719, -3.8281, -1.8281,  ...,  2.1562,  1.3203,  1.4766],\n",
      "         [ 0.3066, -2.9219, -1.6953,  ..., -0.6094,  0.1641,  2.4844],\n",
      "         [ 1.1719, -2.6875, -1.9531,  ..., -1.8438,  0.9336,  1.6328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>)\n",
      ", the  day I I\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# Check forward pass\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "prompt_text = \"Today is the first time that\"\n",
    "tokenized_prompt = tokenizer(prompt_text, return_tensors = 'pt').to(\"cuda\")\n",
    "print(tokenizer.decode(tokenized_prompt.input_ids.squeeze()))\n",
    "output = model(tokenized_prompt.input_ids)\n",
    "out_ids = torch.argmax(output.logits, dim=-1).squeeze()\n",
    "print(tokenizer.decode(out_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE WEIGHTS IN BIN FILE\n",
    "BIN_SAVE_PATH = '/workspace/transformers_zamba2_dev/src/transformers/models/zamba2/state_dict_hf.bin'\n",
    "torch.save(model.state_dict(), BIN_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model from .bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zamba requires an initialized `HybridMambaAttentionDynamicCache` to return a cache. None was provided, so no cache will be returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Today is the first time that\n",
      "torch.Size([7, 1, 32000])\n",
      "tensor([[[-4.1799,  3.0531,  0.4019,  ..., -4.2610, -4.1060, -1.7605]],\n",
      "\n",
      "        [[-7.2152,  6.8994, -1.9752,  ..., -6.3170, -5.7430, -4.1967]],\n",
      "\n",
      "        [[-5.0736,  3.2236, -1.5486,  ..., -4.9293, -4.5989, -3.4123]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-5.2612,  3.3268, -5.0263,  ..., -5.3279, -6.6721, -3.9966]],\n",
      "\n",
      "        [[-6.1001,  4.9477, -0.8853,  ..., -5.5717, -6.0048, -4.3728]],\n",
      "\n",
      "        [[-5.1238,  3.4259, -3.8259,  ..., -4.6562, -5.1366, -2.7277]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "import torch\n",
    "from transformers.models.zamba2 import Zamba2ForCausalLM, Zamba2Config\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Instantiate random Zamba\n",
    "config = Zamba2Config()\n",
    "model = Zamba2ForCausalLM(config).cuda()\n",
    "\n",
    "weights = torch.load('/workspace/transformers_zamba2_dev/src/transformers/models/zamba2/state_dict_hf.bin')\n",
    "model.load_state_dict(weights)\n",
    "# model = model.to(torch.bfloat16)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "prompt_text = \"Today is the first time that\"\n",
    "tokenized_prompt = tokenizer(prompt_text, return_tensors = 'pt').to(\"cuda\")\n",
    "print(tokenizer.decode(tokenized_prompt.input_ids.squeeze()))\n",
    "output = model(tokenized_prompt.input_ids)\n",
    "print(output.logits.shape)\n",
    "print(output.logits)\n",
    "out_ids = torch.argmax(output.logits, dim=-1).transpose(0,1)\n",
    "print(tokenizer.decode(out_ids.squeeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zamba requires an initialized `HybridMambaAttentionDynamicCache` to return a cache. None was provided, so no cache will be returned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Today is the first time that\n",
      ", the  day I I\n"
     ]
    }
   ],
   "source": [
    "# Check forward pass\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "prompt_text = \"Today is the first time that\"\n",
    "tokenized_prompt = tokenizer(prompt_text, return_tensors = 'pt').to(\"cuda\")\n",
    "print(tokenizer.decode(tokenized_prompt.input_ids.squeeze()))\n",
    "output = model(tokenized_prompt.input_ids)\n",
    "out_ids = torch.argmax(output.logits, dim=-1).squeeze()\n",
    "print(tokenizer.decode(out_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
