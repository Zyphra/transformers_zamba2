{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate small random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zarr-based strategies will not be registered because of missing packages\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mamba_model import MambaModel\n",
    "from mamba_config import MambaConfig\n",
    "import torch\n",
    "\n",
    "# Random init model\n",
    "\n",
    "config = MambaConfig(\n",
    "    num_layers = 3, # 5\n",
    "    hidden_size = 256, # 1024\n",
    "    mamba_headdim = 64,\n",
    "    mamba_ngroups = 1,\n",
    "    state_size = 16,\n",
    "    conv_dimension = 4,\n",
    "    expansion_factor = 2,\n",
    "    rms_norm = True,\n",
    "    bias = False,\n",
    "    use_mem_mlp = False, #True,\n",
    "    use_mem_rope =  False, #True,\n",
    "    num_attention_heads = 16,\n",
    "    num_mem_heads = 0, #16,\n",
    "    num_mem_blocks = 0, #2,\n",
    "    vocab_size = 32000,\n",
    "    layer_mapping = [\"m\", \"m\", \"m\"]\n",
    ")\n",
    "\n",
    "model = MambaModel(config = config, max_sequence_length = 4096)\n",
    "model = model.cuda().half()\n",
    "# Tokenizer\n",
    "from megatron.tokenizer.tokenizer import _HFAutoTokenizer\n",
    "from megatron.text_generation.tokenization import tokenize_prompts\n",
    "tokenizer = _HFAutoTokenizer(\"mistralai/Mistral-7B-v0.1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load 3B checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--base-model-type', 'mamba', '--num-layers', '54', '--mamba-headdim', '64', '--mamba-ngroups', '1', '--hidden-size', '2560', '--state-size', '64', '--conv-dimension', '4', '--expansion-factor', '2', '--seq-length', '4096', '--max-position-embeddings', '4096', '--micro-batch-size', '3', '--global-batch-size', '384', '--lr', '2.0e-4', '--train-iters', '1_700_000', '--lr-decay-iters', '1_700_000', '--lr-decay-style', 'cosine', '--min-lr', '5.0e-5', '--lr-warmup-init', '0.0', '--weight-decay', '0.1', '--adam-beta2', '0.95', '--lr-warmup-iters', '5000', '--clip-grad', '1.0', '--bf16', '--recompute-granularity', 'selective', '--accumulate-allreduce-grads-in-fp32', '--attention-dropout', '0.0', '--hidden-dropout', '0.0', '--rms-norm', 'True', '--disable-bias-linear', '--num-mem-heads', '32', '--use-mem-mlp', '--num-attention-heads', '32', '--num-mem-blocks', '2', '--use-shared-block-lora', '--lora-rank', '128', '--use-distributed-optimizer', '--mamba-moe-layers', 'm', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'g', 'm', 'm', '--buffer-params', '96']\n",
      "odict_keys(['embedding.weight', 'decoder.layers.0.mixer.dt_bias', 'decoder.layers.0.mixer.A_log', 'decoder.layers.0.mixer.D', 'decoder.layers.0.mixer.in_proj.0.weight', 'decoder.layers.0.mixer.conv1d.weight', 'decoder.layers.0.mixer.conv1d.bias', 'decoder.layers.0.mixer.norm.weight', 'decoder.layers.0.mixer.out_proj.weight', 'decoder.layers.0.norm.weight', 'decoder.layers.1.mixer.dt_bias', 'decoder.layers.1.mixer.A_log', 'decoder.layers.1.mixer.D', 'decoder.layers.1.mixer.in_proj.0.weight', 'decoder.layers.1.mixer.conv1d.weight', 'decoder.layers.1.mixer.conv1d.bias', 'decoder.layers.1.mixer.norm.weight', 'decoder.layers.1.mixer.out_proj.weight', 'decoder.layers.1.norm.weight', 'decoder.layers.2.mixer.dt_bias', 'decoder.layers.2.mixer.A_log', 'decoder.layers.2.mixer.D', 'decoder.layers.2.mixer.in_proj.0.weight', 'decoder.layers.2.mixer.conv1d.weight', 'decoder.layers.2.mixer.conv1d.bias', 'decoder.layers.2.mixer.norm.weight', 'decoder.layers.2.mixer.out_proj.weight', 'decoder.layers.2.norm.weight', 'decoder.layers.3.mixer.dt_bias', 'decoder.layers.3.mixer.A_log', 'decoder.layers.3.mixer.D', 'decoder.layers.3.mixer.in_proj.0.weight', 'decoder.layers.3.mixer.conv1d.weight', 'decoder.layers.3.mixer.conv1d.bias', 'decoder.layers.3.mixer.norm.weight', 'decoder.layers.3.mixer.out_proj.weight', 'decoder.layers.3.norm.weight', 'decoder.layers.4.mixer.dt_bias', 'decoder.layers.4.mixer.A_log', 'decoder.layers.4.mixer.D', 'decoder.layers.4.mixer.in_proj.0.weight', 'decoder.layers.4.mixer.conv1d.weight', 'decoder.layers.4.mixer.conv1d.bias', 'decoder.layers.4.mixer.norm.weight', 'decoder.layers.4.mixer.out_proj.weight', 'decoder.layers.4.norm.weight', 'decoder.layers.5.mixer.dt_bias', 'decoder.layers.5.mixer.A_log', 'decoder.layers.5.mixer.D', 'decoder.layers.5.mixer.in_proj.0.weight', 'decoder.layers.5.mixer.conv1d.weight', 'decoder.layers.5.mixer.conv1d.bias', 'decoder.layers.5.mixer.norm.weight', 'decoder.layers.5.mixer.out_proj.weight', 'decoder.layers.5.norm.weight', 'decoder.layers.6.mixer.dt_bias', 'decoder.layers.6.mixer.A_log', 'decoder.layers.6.mixer.D', 'decoder.layers.6.mixer.in_proj.0.weight', 'decoder.layers.6.mixer.conv1d.weight', 'decoder.layers.6.mixer.conv1d.bias', 'decoder.layers.6.mixer.norm.weight', 'decoder.layers.6.mixer.out_proj.weight', 'decoder.layers.6.norm.weight', 'decoder.layers.7.mixer.dt_bias', 'decoder.layers.7.mixer.A_log', 'decoder.layers.7.mixer.D', 'decoder.layers.7.mixer.in_proj.0.weight', 'decoder.layers.7.mixer.conv1d.weight', 'decoder.layers.7.mixer.conv1d.bias', 'decoder.layers.7.mixer.norm.weight', 'decoder.layers.7.mixer.out_proj.weight', 'decoder.layers.7.norm.weight', 'decoder.layers.8.mixer.dt_bias', 'decoder.layers.8.mixer.A_log', 'decoder.layers.8.mixer.D', 'decoder.layers.8.mixer.in_proj.0.weight', 'decoder.layers.8.mixer.conv1d.weight', 'decoder.layers.8.mixer.conv1d.bias', 'decoder.layers.8.mixer.norm.weight', 'decoder.layers.8.mixer.out_proj.weight', 'decoder.layers.8.norm.weight', 'decoder.layers.9.mixer.dt_bias', 'decoder.layers.9.mixer.A_log', 'decoder.layers.9.mixer.D', 'decoder.layers.9.mixer.in_proj.0.weight', 'decoder.layers.9.mixer.conv1d.weight', 'decoder.layers.9.mixer.conv1d.bias', 'decoder.layers.9.mixer.norm.weight', 'decoder.layers.9.mixer.out_proj.weight', 'decoder.layers.9.norm.weight', 'decoder.layers.10.mixer.dt_bias', 'decoder.layers.10.mixer.A_log', 'decoder.layers.10.mixer.D', 'decoder.layers.10.mixer.in_proj.0.weight', 'decoder.layers.10.mixer.conv1d.weight', 'decoder.layers.10.mixer.conv1d.bias', 'decoder.layers.10.mixer.norm.weight', 'decoder.layers.10.mixer.out_proj.weight', 'decoder.layers.10.norm.weight', 'decoder.layers.11.mixer.dt_bias', 'decoder.layers.11.mixer.A_log', 'decoder.layers.11.mixer.D', 'decoder.layers.11.mixer.in_proj.0.weight', 'decoder.layers.11.mixer.conv1d.weight', 'decoder.layers.11.mixer.conv1d.bias', 'decoder.layers.11.mixer.norm.weight', 'decoder.layers.11.mixer.out_proj.weight', 'decoder.layers.11.norm.weight', 'decoder.layers.12.mixer.dt_bias', 'decoder.layers.12.mixer.A_log', 'decoder.layers.12.mixer.D', 'decoder.layers.12.mixer.in_proj.0.weight', 'decoder.layers.12.mixer.conv1d.weight', 'decoder.layers.12.mixer.conv1d.bias', 'decoder.layers.12.mixer.norm.weight', 'decoder.layers.12.mixer.out_proj.weight', 'decoder.layers.12.norm.weight', 'decoder.layers.13.mixer.dt_bias', 'decoder.layers.13.mixer.A_log', 'decoder.layers.13.mixer.D', 'decoder.layers.13.mixer.in_proj.0.weight', 'decoder.layers.13.mixer.conv1d.weight', 'decoder.layers.13.mixer.conv1d.bias', 'decoder.layers.13.mixer.norm.weight', 'decoder.layers.13.mixer.out_proj.weight', 'decoder.layers.13.norm.weight', 'decoder.layers.14.mixer.dt_bias', 'decoder.layers.14.mixer.A_log', 'decoder.layers.14.mixer.D', 'decoder.layers.14.mixer.in_proj.0.weight', 'decoder.layers.14.mixer.conv1d.weight', 'decoder.layers.14.mixer.conv1d.bias', 'decoder.layers.14.mixer.norm.weight', 'decoder.layers.14.mixer.out_proj.weight', 'decoder.layers.14.norm.weight', 'decoder.layers.15.mixer.dt_bias', 'decoder.layers.15.mixer.A_log', 'decoder.layers.15.mixer.D', 'decoder.layers.15.mixer.in_proj.0.weight', 'decoder.layers.15.mixer.conv1d.weight', 'decoder.layers.15.mixer.conv1d.bias', 'decoder.layers.15.mixer.norm.weight', 'decoder.layers.15.mixer.out_proj.weight', 'decoder.layers.15.norm.weight', 'decoder.layers.16.mixer.dt_bias', 'decoder.layers.16.mixer.A_log', 'decoder.layers.16.mixer.D', 'decoder.layers.16.mixer.in_proj.0.weight', 'decoder.layers.16.mixer.conv1d.weight', 'decoder.layers.16.mixer.conv1d.bias', 'decoder.layers.16.mixer.norm.weight', 'decoder.layers.16.mixer.out_proj.weight', 'decoder.layers.16.norm.weight', 'decoder.layers.17.mixer.dt_bias', 'decoder.layers.17.mixer.A_log', 'decoder.layers.17.mixer.D', 'decoder.layers.17.mixer.in_proj.0.weight', 'decoder.layers.17.mixer.conv1d.weight', 'decoder.layers.17.mixer.conv1d.bias', 'decoder.layers.17.mixer.norm.weight', 'decoder.layers.17.mixer.out_proj.weight', 'decoder.layers.17.norm.weight', 'decoder.layers.18.mixer.dt_bias', 'decoder.layers.18.mixer.A_log', 'decoder.layers.18.mixer.D', 'decoder.layers.18.mixer.in_proj.0.weight', 'decoder.layers.18.mixer.conv1d.weight', 'decoder.layers.18.mixer.conv1d.bias', 'decoder.layers.18.mixer.norm.weight', 'decoder.layers.18.mixer.out_proj.weight', 'decoder.layers.18.norm.weight', 'decoder.layers.19.mixer.dt_bias', 'decoder.layers.19.mixer.A_log', 'decoder.layers.19.mixer.D', 'decoder.layers.19.mixer.in_proj.0.weight', 'decoder.layers.19.mixer.conv1d.weight', 'decoder.layers.19.mixer.conv1d.bias', 'decoder.layers.19.mixer.norm.weight', 'decoder.layers.19.mixer.out_proj.weight', 'decoder.layers.19.norm.weight', 'decoder.layers.20.mixer.dt_bias', 'decoder.layers.20.mixer.A_log', 'decoder.layers.20.mixer.D', 'decoder.layers.20.mixer.in_proj.0.weight', 'decoder.layers.20.mixer.conv1d.weight', 'decoder.layers.20.mixer.conv1d.bias', 'decoder.layers.20.mixer.norm.weight', 'decoder.layers.20.mixer.out_proj.weight', 'decoder.layers.20.norm.weight', 'decoder.layers.21.mixer.dt_bias', 'decoder.layers.21.mixer.A_log', 'decoder.layers.21.mixer.D', 'decoder.layers.21.mixer.in_proj.0.weight', 'decoder.layers.21.mixer.conv1d.weight', 'decoder.layers.21.mixer.conv1d.bias', 'decoder.layers.21.mixer.norm.weight', 'decoder.layers.21.mixer.out_proj.weight', 'decoder.layers.21.norm.weight', 'decoder.layers.22.mixer.dt_bias', 'decoder.layers.22.mixer.A_log', 'decoder.layers.22.mixer.D', 'decoder.layers.22.mixer.in_proj.0.weight', 'decoder.layers.22.mixer.conv1d.weight', 'decoder.layers.22.mixer.conv1d.bias', 'decoder.layers.22.mixer.norm.weight', 'decoder.layers.22.mixer.out_proj.weight', 'decoder.layers.22.norm.weight', 'decoder.layers.23.mixer.dt_bias', 'decoder.layers.23.mixer.A_log', 'decoder.layers.23.mixer.D', 'decoder.layers.23.mixer.in_proj.0.weight', 'decoder.layers.23.mixer.conv1d.weight', 'decoder.layers.23.mixer.conv1d.bias', 'decoder.layers.23.mixer.norm.weight', 'decoder.layers.23.mixer.out_proj.weight', 'decoder.layers.23.norm.weight', 'decoder.layers.24.mixer.dt_bias', 'decoder.layers.24.mixer.A_log', 'decoder.layers.24.mixer.D', 'decoder.layers.24.mixer.in_proj.0.weight', 'decoder.layers.24.mixer.conv1d.weight', 'decoder.layers.24.mixer.conv1d.bias', 'decoder.layers.24.mixer.norm.weight', 'decoder.layers.24.mixer.out_proj.weight', 'decoder.layers.24.norm.weight', 'decoder.layers.25.mixer.dt_bias', 'decoder.layers.25.mixer.A_log', 'decoder.layers.25.mixer.D', 'decoder.layers.25.mixer.in_proj.0.weight', 'decoder.layers.25.mixer.conv1d.weight', 'decoder.layers.25.mixer.conv1d.bias', 'decoder.layers.25.mixer.norm.weight', 'decoder.layers.25.mixer.out_proj.weight', 'decoder.layers.25.norm.weight', 'decoder.layers.26.mixer.dt_bias', 'decoder.layers.26.mixer.A_log', 'decoder.layers.26.mixer.D', 'decoder.layers.26.mixer.in_proj.0.weight', 'decoder.layers.26.mixer.conv1d.weight', 'decoder.layers.26.mixer.conv1d.bias', 'decoder.layers.26.mixer.norm.weight', 'decoder.layers.26.mixer.out_proj.weight', 'decoder.layers.26.norm.weight', 'decoder.layers.27.mixer.dt_bias', 'decoder.layers.27.mixer.A_log', 'decoder.layers.27.mixer.D', 'decoder.layers.27.mixer.in_proj.0.weight', 'decoder.layers.27.mixer.conv1d.weight', 'decoder.layers.27.mixer.conv1d.bias', 'decoder.layers.27.mixer.norm.weight', 'decoder.layers.27.mixer.out_proj.weight', 'decoder.layers.27.norm.weight', 'decoder.layers.28.mixer.dt_bias', 'decoder.layers.28.mixer.A_log', 'decoder.layers.28.mixer.D', 'decoder.layers.28.mixer.in_proj.0.weight', 'decoder.layers.28.mixer.conv1d.weight', 'decoder.layers.28.mixer.conv1d.bias', 'decoder.layers.28.mixer.norm.weight', 'decoder.layers.28.mixer.out_proj.weight', 'decoder.layers.28.norm.weight', 'decoder.layers.29.mixer.dt_bias', 'decoder.layers.29.mixer.A_log', 'decoder.layers.29.mixer.D', 'decoder.layers.29.mixer.in_proj.0.weight', 'decoder.layers.29.mixer.conv1d.weight', 'decoder.layers.29.mixer.conv1d.bias', 'decoder.layers.29.mixer.norm.weight', 'decoder.layers.29.mixer.out_proj.weight', 'decoder.layers.29.norm.weight', 'decoder.layers.30.mixer.dt_bias', 'decoder.layers.30.mixer.A_log', 'decoder.layers.30.mixer.D', 'decoder.layers.30.mixer.in_proj.0.weight', 'decoder.layers.30.mixer.conv1d.weight', 'decoder.layers.30.mixer.conv1d.bias', 'decoder.layers.30.mixer.norm.weight', 'decoder.layers.30.mixer.out_proj.weight', 'decoder.layers.30.norm.weight', 'decoder.layers.31.mixer.dt_bias', 'decoder.layers.31.mixer.A_log', 'decoder.layers.31.mixer.D', 'decoder.layers.31.mixer.in_proj.0.weight', 'decoder.layers.31.mixer.conv1d.weight', 'decoder.layers.31.mixer.conv1d.bias', 'decoder.layers.31.mixer.norm.weight', 'decoder.layers.31.mixer.out_proj.weight', 'decoder.layers.31.norm.weight', 'decoder.layers.32.mixer.dt_bias', 'decoder.layers.32.mixer.A_log', 'decoder.layers.32.mixer.D', 'decoder.layers.32.mixer.in_proj.0.weight', 'decoder.layers.32.mixer.conv1d.weight', 'decoder.layers.32.mixer.conv1d.bias', 'decoder.layers.32.mixer.norm.weight', 'decoder.layers.32.mixer.out_proj.weight', 'decoder.layers.32.norm.weight', 'decoder.layers.33.mixer.dt_bias', 'decoder.layers.33.mixer.A_log', 'decoder.layers.33.mixer.D', 'decoder.layers.33.mixer.in_proj.0.weight', 'decoder.layers.33.mixer.conv1d.weight', 'decoder.layers.33.mixer.conv1d.bias', 'decoder.layers.33.mixer.norm.weight', 'decoder.layers.33.mixer.out_proj.weight', 'decoder.layers.33.norm.weight', 'decoder.layers.34.mixer.dt_bias', 'decoder.layers.34.mixer.A_log', 'decoder.layers.34.mixer.D', 'decoder.layers.34.mixer.in_proj.0.weight', 'decoder.layers.34.mixer.conv1d.weight', 'decoder.layers.34.mixer.conv1d.bias', 'decoder.layers.34.mixer.norm.weight', 'decoder.layers.34.mixer.out_proj.weight', 'decoder.layers.34.norm.weight', 'decoder.layers.35.mixer.dt_bias', 'decoder.layers.35.mixer.A_log', 'decoder.layers.35.mixer.D', 'decoder.layers.35.mixer.in_proj.0.weight', 'decoder.layers.35.mixer.conv1d.weight', 'decoder.layers.35.mixer.conv1d.bias', 'decoder.layers.35.mixer.norm.weight', 'decoder.layers.35.mixer.out_proj.weight', 'decoder.layers.35.norm.weight', 'decoder.layers.36.mixer.dt_bias', 'decoder.layers.36.mixer.A_log', 'decoder.layers.36.mixer.D', 'decoder.layers.36.mixer.in_proj.0.weight', 'decoder.layers.36.mixer.conv1d.weight', 'decoder.layers.36.mixer.conv1d.bias', 'decoder.layers.36.mixer.norm.weight', 'decoder.layers.36.mixer.out_proj.weight', 'decoder.layers.36.norm.weight', 'decoder.layers.37.mixer.dt_bias', 'decoder.layers.37.mixer.A_log', 'decoder.layers.37.mixer.D', 'decoder.layers.37.mixer.in_proj.0.weight', 'decoder.layers.37.mixer.conv1d.weight', 'decoder.layers.37.mixer.conv1d.bias', 'decoder.layers.37.mixer.norm.weight', 'decoder.layers.37.mixer.out_proj.weight', 'decoder.layers.37.norm.weight', 'decoder.layers.38.mixer.dt_bias', 'decoder.layers.38.mixer.A_log', 'decoder.layers.38.mixer.D', 'decoder.layers.38.mixer.in_proj.0.weight', 'decoder.layers.38.mixer.conv1d.weight', 'decoder.layers.38.mixer.conv1d.bias', 'decoder.layers.38.mixer.norm.weight', 'decoder.layers.38.mixer.out_proj.weight', 'decoder.layers.38.norm.weight', 'decoder.layers.39.mixer.dt_bias', 'decoder.layers.39.mixer.A_log', 'decoder.layers.39.mixer.D', 'decoder.layers.39.mixer.in_proj.0.weight', 'decoder.layers.39.mixer.conv1d.weight', 'decoder.layers.39.mixer.conv1d.bias', 'decoder.layers.39.mixer.norm.weight', 'decoder.layers.39.mixer.out_proj.weight', 'decoder.layers.39.norm.weight', 'decoder.layers.40.mixer.dt_bias', 'decoder.layers.40.mixer.A_log', 'decoder.layers.40.mixer.D', 'decoder.layers.40.mixer.in_proj.0.weight', 'decoder.layers.40.mixer.conv1d.weight', 'decoder.layers.40.mixer.conv1d.bias', 'decoder.layers.40.mixer.norm.weight', 'decoder.layers.40.mixer.out_proj.weight', 'decoder.layers.40.norm.weight', 'decoder.layers.41.mixer.dt_bias', 'decoder.layers.41.mixer.A_log', 'decoder.layers.41.mixer.D', 'decoder.layers.41.mixer.in_proj.0.weight', 'decoder.layers.41.mixer.conv1d.weight', 'decoder.layers.41.mixer.conv1d.bias', 'decoder.layers.41.mixer.norm.weight', 'decoder.layers.41.mixer.out_proj.weight', 'decoder.layers.41.norm.weight', 'decoder.layers.42.mixer.dt_bias', 'decoder.layers.42.mixer.A_log', 'decoder.layers.42.mixer.D', 'decoder.layers.42.mixer.in_proj.0.weight', 'decoder.layers.42.mixer.conv1d.weight', 'decoder.layers.42.mixer.conv1d.bias', 'decoder.layers.42.mixer.norm.weight', 'decoder.layers.42.mixer.out_proj.weight', 'decoder.layers.42.norm.weight', 'decoder.layers.43.mixer.dt_bias', 'decoder.layers.43.mixer.A_log', 'decoder.layers.43.mixer.D', 'decoder.layers.43.mixer.in_proj.0.weight', 'decoder.layers.43.mixer.conv1d.weight', 'decoder.layers.43.mixer.conv1d.bias', 'decoder.layers.43.mixer.norm.weight', 'decoder.layers.43.mixer.out_proj.weight', 'decoder.layers.43.norm.weight', 'decoder.layers.44.mixer.dt_bias', 'decoder.layers.44.mixer.A_log', 'decoder.layers.44.mixer.D', 'decoder.layers.44.mixer.in_proj.0.weight', 'decoder.layers.44.mixer.conv1d.weight', 'decoder.layers.44.mixer.conv1d.bias', 'decoder.layers.44.mixer.norm.weight', 'decoder.layers.44.mixer.out_proj.weight', 'decoder.layers.44.norm.weight', 'decoder.layers.45.mixer.dt_bias', 'decoder.layers.45.mixer.A_log', 'decoder.layers.45.mixer.D', 'decoder.layers.45.mixer.in_proj.0.weight', 'decoder.layers.45.mixer.conv1d.weight', 'decoder.layers.45.mixer.conv1d.bias', 'decoder.layers.45.mixer.norm.weight', 'decoder.layers.45.mixer.out_proj.weight', 'decoder.layers.45.norm.weight', 'decoder.layers.46.mixer.dt_bias', 'decoder.layers.46.mixer.A_log', 'decoder.layers.46.mixer.D', 'decoder.layers.46.mixer.in_proj.0.weight', 'decoder.layers.46.mixer.conv1d.weight', 'decoder.layers.46.mixer.conv1d.bias', 'decoder.layers.46.mixer.norm.weight', 'decoder.layers.46.mixer.out_proj.weight', 'decoder.layers.46.norm.weight', 'decoder.layers.47.mixer.dt_bias', 'decoder.layers.47.mixer.A_log', 'decoder.layers.47.mixer.D', 'decoder.layers.47.mixer.in_proj.0.weight', 'decoder.layers.47.mixer.conv1d.weight', 'decoder.layers.47.mixer.conv1d.bias', 'decoder.layers.47.mixer.norm.weight', 'decoder.layers.47.mixer.out_proj.weight', 'decoder.layers.47.norm.weight', 'decoder.layers.48.mixer.dt_bias', 'decoder.layers.48.mixer.A_log', 'decoder.layers.48.mixer.D', 'decoder.layers.48.mixer.in_proj.0.weight', 'decoder.layers.48.mixer.conv1d.weight', 'decoder.layers.48.mixer.conv1d.bias', 'decoder.layers.48.mixer.norm.weight', 'decoder.layers.48.mixer.out_proj.weight', 'decoder.layers.48.norm.weight', 'decoder.layers.49.mixer.dt_bias', 'decoder.layers.49.mixer.A_log', 'decoder.layers.49.mixer.D', 'decoder.layers.49.mixer.in_proj.0.weight', 'decoder.layers.49.mixer.conv1d.weight', 'decoder.layers.49.mixer.conv1d.bias', 'decoder.layers.49.mixer.norm.weight', 'decoder.layers.49.mixer.out_proj.weight', 'decoder.layers.49.norm.weight', 'decoder.layers.50.mixer.dt_bias', 'decoder.layers.50.mixer.A_log', 'decoder.layers.50.mixer.D', 'decoder.layers.50.mixer.in_proj.0.weight', 'decoder.layers.50.mixer.conv1d.weight', 'decoder.layers.50.mixer.conv1d.bias', 'decoder.layers.50.mixer.norm.weight', 'decoder.layers.50.mixer.out_proj.weight', 'decoder.layers.50.norm.weight', 'decoder.layers.51.mixer.dt_bias', 'decoder.layers.51.mixer.A_log', 'decoder.layers.51.mixer.D', 'decoder.layers.51.mixer.in_proj.0.weight', 'decoder.layers.51.mixer.conv1d.weight', 'decoder.layers.51.mixer.conv1d.bias', 'decoder.layers.51.mixer.norm.weight', 'decoder.layers.51.mixer.out_proj.weight', 'decoder.layers.51.norm.weight', 'decoder.layers.52.mixer.dt_bias', 'decoder.layers.52.mixer.A_log', 'decoder.layers.52.mixer.D', 'decoder.layers.52.mixer.in_proj.0.weight', 'decoder.layers.52.mixer.conv1d.weight', 'decoder.layers.52.mixer.conv1d.bias', 'decoder.layers.52.mixer.norm.weight', 'decoder.layers.52.mixer.out_proj.weight', 'decoder.layers.52.norm.weight', 'decoder.layers.53.mixer.dt_bias', 'decoder.layers.53.mixer.A_log', 'decoder.layers.53.mixer.D', 'decoder.layers.53.mixer.in_proj.0.weight', 'decoder.layers.53.mixer.conv1d.weight', 'decoder.layers.53.mixer.conv1d.bias', 'decoder.layers.53.mixer.norm.weight', 'decoder.layers.53.mixer.out_proj.weight', 'decoder.layers.53.norm.weight', 'decoder.blocks.0.sa.mixer.linear_qkv.weight', 'decoder.blocks.0.sa.mixer.linear_proj.weight', 'decoder.blocks.0.sa.norm.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1.weight', 'decoder.blocks.0.mlp.mixer.linear_fc2.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.0.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.1.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.2.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.3.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.4.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.5.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.6.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.7.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_A_list.8.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.0.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.1.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.2.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.3.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.4.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.5.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.6.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.7.weight', 'decoder.blocks.0.mlp.mixer.linear_fc1_lora_B_list.8.weight', 'decoder.blocks.0.mlp.norm.weight', 'decoder.blocks.1.sa.mixer.linear_qkv.weight', 'decoder.blocks.1.sa.mixer.linear_proj.weight', 'decoder.blocks.1.sa.norm.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1.weight', 'decoder.blocks.1.mlp.mixer.linear_fc2.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.0.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.1.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.2.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.3.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.4.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.5.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.6.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.7.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_A_list.8.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_B_list.0.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_B_list.1.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_B_list.2.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_B_list.3.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_B_list.4.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_B_list.5.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_B_list.6.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_B_list.7.weight', 'decoder.blocks.1.mlp.mixer.linear_fc1_lora_B_list.8.weight', 'decoder.blocks.1.mlp.norm.weight', 'decoder.block_map.6.weight', 'decoder.block_map.12.weight', 'decoder.block_map.18.weight', 'decoder.block_map.24.weight', 'decoder.block_map.30.weight', 'decoder.block_map.36.weight', 'decoder.block_map.42.weight', 'decoder.block_map.47.weight', 'decoder.block_map.51.weight', 'decoder.final_layernorm.weight'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zarr-based strategies will not be registered because of missing packages\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "from mamba_model import MambaModel\n",
    "from mamba_config import MambaConfig\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "# checkpoint_name = \"/workspace/zamba2_tiny_test/iter_0000015/mp_rank_00/model_optim_rng.pt\"\n",
    "checkpoint_name = \"/checkpoints/zamba_3B_attempt_2/iter_1235000/mp_rank_00/model_optim_rng.pt\"\n",
    "state_dict = torch.load(checkpoint_name, map_location = \"cpu\")\n",
    "# config_path = \"/workspace/zamba2_tiny_test/config.sh\"\n",
    "config_path = \"/workspace/Mamba-MoE/examples/mamba_memory_block/zamba_3B_attempt3.sh\"\n",
    "\n",
    "def extract_keyword_args(filestr, keyword):\n",
    "    gpt_split = filestr.split(keyword)\n",
    "    if len(gpt_split) <=1:\n",
    "        raise ValueError(\"Config provided does not have a GPT_ARGS variable provided\")\n",
    "    arg_splits = gpt_split[1].split(\"\\\"\")\n",
    "    gpt_args = arg_splits[1]\n",
    "    gpt_args = gpt_args.replace(\"\\n\",\"\").replace(\"\\\\\",\"\").replace(\"\\t\",\"\")\n",
    "    gpt_args = ' '.join(gpt_args.split())\n",
    "    return gpt_args.strip().split(\" \")\n",
    "    \n",
    "with open(config_path,\"r\") as f:\n",
    "        filestr = f.read()\n",
    "config_args = extract_keyword_args(filestr, \"GPT_ARGS\")\n",
    "print(config_args)\n",
    "\n",
    "config = MambaConfig(\n",
    "    num_layers = 54,\n",
    "    hidden_size = 2560,\n",
    "    mamba_headdim = 64,\n",
    "    mamba_ngroups = 1,\n",
    "    state_size = 64,\n",
    "    conv_dimension = 4,\n",
    "    expansion_factor = 2,\n",
    "    rms_norm = True,\n",
    "    bias = False,\n",
    "    use_mem_mlp = True,\n",
    "    num_attention_heads = 32,\n",
    "    num_mem_heads = 32,\n",
    "    num_mem_blocks = 2,\n",
    "    use_shared_block_lora = True,\n",
    "    lora_rank = 128,\n",
    "    vocab_size = 32000,\n",
    "    layer_mapping = ['m', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'm', 'g', 'm', 'm', 'm', 'g', 'm', 'm']\n",
    "    \n",
    ")\n",
    "#print(config[\"num_layers\"])\n",
    "# #layer_mapping = [\"r\", \"r\", \"g\", \"r\", \"r\"]\n",
    "model = MambaModel(config = config, max_sequence_length = 4096)\n",
    "\n",
    "checkpoint_state_dict = dict(copy.deepcopy(state_dict[\"model\"]))\n",
    "model_state_dict = model.state_dict()\n",
    "del checkpoint_state_dict[\"embedding.position_embeddings.weight\"] # delete pos embeddings from the state dict\n",
    "checkpoint_state_dict[\"embedding.weight\"] = checkpoint_state_dict[\"embedding.word_embeddings.weight\"].clone()\n",
    "del checkpoint_state_dict[\"embedding.word_embeddings.weight\"]\n",
    "# delete the extra state\n",
    "for k in list(checkpoint_state_dict.keys()):\n",
    "    if \"extra_state\" in k:\n",
    "        del checkpoint_state_dict[k]\n",
    "    if \"buffer_params\" in k:\n",
    "        del checkpoint_state_dict[k]\n",
    "        # okay let's leave this out. I was wanting to do key matching\n",
    "# for (k_c, k_m) in zip(checkpoint_state_dict.keys(), model_state_dict.keys()):\n",
    "#     model_state_dict[k_m].data = checkpoint_state_dict[k_c].clone().data\n",
    "#     #print(checkpoint_state_dict[k_c].clone().data.dtype,checkpoint_state_dict[k_c].clone().data.device)\n",
    "print(model_state_dict.keys())\n",
    "model.load_state_dict(checkpoint_state_dict)\n",
    "model = model.cuda()#.float()\n",
    "# Tokenizer\n",
    "from megatron.tokenizer.tokenizer import _HFAutoTokenizer\n",
    "from megatron.text_generation.tokenization import tokenize_prompts\n",
    "tokenizer = _HFAutoTokenizer(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy generation without cache\n",
    "prompt = 'Hello, how are you?'\n",
    "tokens_to_generate = 10\n",
    "\n",
    "prompts_tokens = [tokenizer.tokenize(prompt)]\n",
    "text_ids = torch.cuda.LongTensor(prompts_tokens).transpose(0, 1)\n",
    "print(text_ids.shape)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(tokens_to_generate):\n",
    "        out = model(text_ids)\n",
    "        out_last = out[:, -1]\n",
    "        id = torch.argmax(out_last)[None, None]\n",
    "        text_ids = torch.cat((text_ids, id), dim=0)\n",
    "print(text_ids.shape)\n",
    "text_ids = text_ids.transpose(0, 1)[0]\n",
    "tokenizer.detokenize(text_ids.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from megatron/text_generation/forward_step.py\n",
    "# and megatron/core/inference_params.py\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "class InferenceParams:\n",
    "    \"\"\"Inference parameters that are passed to the main model in order\n",
    "    to efficienly calculate and store the context during inference.\"\"\"\n",
    "\n",
    "    def __init__(self, max_batch_size, max_sequence_length):\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.sequence_len_offset = 0\n",
    "        self.batch_size_offset = 0\n",
    "        self.key_value_memory_dict = {}\n",
    "        self.key_value_memory_dict_mamba = {}\n",
    "\n",
    "    def swap_key_value_dict(self, batch_idx):\n",
    "        \"swap between batches\"\n",
    "        if len(self.key_value_memory_dict) == 0:\n",
    "            raise ValueError(\"should not swap when dict in empty\")\n",
    "\n",
    "        for layer_number in self.key_value_memory_dict.keys():\n",
    "            inference_key_memory, inference_value_memory = self.key_value_memory_dict[layer_number]\n",
    "            assert (\n",
    "                len(batch_idx) == inference_key_memory.shape[1]\n",
    "            )  # make sure batch size is the same\n",
    "            new_inference_key_memory = inference_key_memory[:, batch_idx]\n",
    "            new_inference_value_memory = inference_value_memory[:, batch_idx]\n",
    "            self.key_value_memory_dict[layer_number] = (\n",
    "                new_inference_key_memory,\n",
    "                new_inference_value_memory,\n",
    "            )\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "class ForwardStep:\n",
    "    \"\"\"Forward step function.\n",
    "    We use a class here to hide the inference parameters\n",
    "    from the outside caller.\"\"\"\n",
    "\n",
    "    def __init__(self, model, max_batch_size, max_sequence_length):\n",
    "        \"\"\"Set values so we don't need to do it multiple times.\"\"\"\n",
    "        # Make sure model is in eval mode.\n",
    "        assert not isinstance(model, Iterable), \\\n",
    "            'interleaving schedule is not supported for inference'\n",
    "        model.eval()\n",
    "        self.model = model\n",
    "        # Initialize inference parameters.\n",
    "        self.inference_params = InferenceParams(max_batch_size,\n",
    "                                                max_sequence_length)\n",
    "\n",
    "\n",
    "    def __call__(self, tokens):\n",
    "        \"\"\"Invocation of the forward methods. Note that self.inference_params\n",
    "        is being modified by the forward step.\"\"\"\n",
    "        # Run a simple forward pass.\n",
    "        logits = model(tokens, inference_params=self.inference_params)\n",
    "        \n",
    "        # if self.inference_params.sequence_len_offset > 0:\n",
    "        #     for k in range(1, 50):\n",
    "        #         temp_list = list(self.inference_params.key_value_memory_dict_mamba[k])\n",
    "        #         temp_list[0] = temp_list[0] * 0\n",
    "        #         temp_list[1] = temp_list[1] * 0\n",
    "        #         self.inference_params.key_value_memory_dict_mamba[k] = tuple(temp_list)\n",
    "        \n",
    "        # Update the sequence length offset.\n",
    "        self.inference_params.sequence_len_offset += tokens.size(0)\n",
    "\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Hello, how are you?\\nI am fine, thank you.\\nHow'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# greedy generation with cache\n",
    "\n",
    "prompt = 'Hello, how are you?'\n",
    "tokens_to_generate = 10\n",
    "batch_size = 1\n",
    "\n",
    "prompts_tokens = [tokenizer.tokenize(prompt)]\n",
    "text_ids = torch.cuda.LongTensor(prompts_tokens).transpose(0, 1)\n",
    "prompt_length = text_ids.shape[0]\n",
    "max_sequence_length = prompt_length + tokens_to_generate\n",
    "\n",
    "# allocate tensor for full sequence\n",
    "tokens = torch.zeros((max_sequence_length, batch_size), dtype=torch.int64, device=torch.cuda.current_device())\n",
    "tokens[:prompt_length].copy_(text_ids)\n",
    "\n",
    "forward_step = ForwardStep(model, batch_size, max_sequence_length)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prev_context_length = 0\n",
    "    for context_length in range(prompt_length, max_sequence_length):\n",
    "        # Pick the slice that we need to pass through the network.\n",
    "        tokens2use = tokens[prev_context_length:context_length]\n",
    "        logits = forward_step(tokens2use)\n",
    "        last_token_logits = logits[:, -1]\n",
    "        new_id = torch.argmax(last_token_logits)\n",
    "        tokens[context_length, 0] = new_id\n",
    "        prev_context_length = context_length\n",
    "\n",
    "print(tokens.shape)\n",
    "tokens = tokens.transpose(0, 1)[0]\n",
    "tokenizer.detokenize(tokens.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize rope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29, 1])\n",
      "torch.Size([1, 29, 32000])\n",
      "tensor([[-4.7043,  4.1115, -1.6878,  ..., -4.9308, -4.6628, -3.5261],\n",
      "        [-7.4544,  1.9833, -5.7091,  ..., -6.7184, -5.4518, -6.2321],\n",
      "        [-7.2265,  0.9190, -5.1137,  ..., -5.7556, -4.5535, -5.0144],\n",
      "        ...,\n",
      "        [-6.3463,  2.1663, -5.2459,  ..., -7.2303, -5.1301, -6.2166],\n",
      "        [-7.8079,  2.8875, -7.0855,  ..., -8.5312, -6.3525, -6.8940],\n",
      "        [-8.7074,  5.8500, -5.4719,  ..., -8.3026, -6.2768, -6.7983]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Forward pass with old rope\n",
    "prompt = 'This is a prompt that I am writing just to check that the output logits of the new rope implementation agree with those of the old implementation'\n",
    "\n",
    "prompts_tokens = [tokenizer.tokenize(prompt)]\n",
    "text_ids = torch.cuda.LongTensor(prompts_tokens).transpose(0, 1)\n",
    "print(text_ids.shape)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(text_ids)\n",
    "print(out.shape)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116442/2377630929.py:5: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  text_ids = torch.cuda.LongTensor(prompts_tokens).transpose(0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29, 1])\n",
      "torch.Size([1, 29, 32000])\n",
      "tensor([[-4.7043,  4.1115, -1.6878,  ..., -4.9308, -4.6628, -3.5261],\n",
      "        [-7.4544,  1.9833, -5.7091,  ..., -6.7184, -5.4518, -6.2321],\n",
      "        [-7.2265,  0.9190, -5.1137,  ..., -5.7556, -4.5535, -5.0144],\n",
      "        ...,\n",
      "        [-6.3463,  2.1663, -5.2459,  ..., -7.2303, -5.1301, -6.2166],\n",
      "        [-7.8079,  2.8875, -7.0855,  ..., -8.5312, -6.3525, -6.8940],\n",
      "        [-8.7074,  5.8500, -5.4719,  ..., -8.3026, -6.2768, -6.7983]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Forward pass with new rope\n",
    "prompt = 'This is a prompt that I am writing just to check that the output logits of the new rope implementation agree with those of the old implementation'\n",
    "\n",
    "prompts_tokens = [tokenizer.tokenize(prompt)]\n",
    "text_ids = torch.cuda.LongTensor(prompts_tokens).transpose(0, 1)\n",
    "print(text_ids.shape)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(text_ids)\n",
    "print(out.shape)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7]) 7\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Hello, how are you?'\n",
    "tokens_to_generate = 10\n",
    "return_output_log_probs = False\n",
    "top_k_sampling = 1\n",
    "top_p_sampling=0.0,\n",
    "top_p_decay=0.0,\n",
    "top_p_bound=0.0,\n",
    "temperature=1.0,\n",
    "use_eod_token_for_early_termination=True,\n",
    "stop_on_double_eol=False,\n",
    "stop_on_eol=False,\n",
    "prevent_newline_after_colon=False,\n",
    "random_seed=-1,\n",
    "return_logits=False\n",
    "\n",
    "# some lines below are adapted from Mamba-MoE/tasks/msdp/prompt.py and api.py\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "context_tokens_tensor = [tokenizer.tokenize(prompt)]\n",
    "context_tokens_tensor = torch.cuda.LongTensor(context_tokens_tensor)\n",
    "context_length_tensor = context_tokens_tensor.shape[1]\n",
    "print(context_tokens_tensor.shape, context_length_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified generation\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_tokens_probs_and_return_on_first_stage(\n",
    "        model, tokens, lengths,\n",
    "        return_output_log_probs=False,\n",
    "        top_k=0, top_p=0.0, top_p_decay=0.0, top_p_bound=0.0,\n",
    "        temperature=1.0,\n",
    "        use_eod_token_for_early_termination=True,\n",
    "        stop_on_double_eol=False,\n",
    "        stop_on_eol=False,\n",
    "        prevent_newline_after_colon=True\n",
    "        ):\n",
    "    \"\"\"Main token generation function.\n",
    "    Arguments:\n",
    "        model: no interleaving is supported.\n",
    "        tokens: prompt tokens extended to be of size [b, max-sequence-length]\n",
    "        lengths: original prompt length, size: [b]\n",
    "        return_output_log_probs: flag to calculate the log probability of\n",
    "            the generated tokens. Note that the log probability is the one\n",
    "            from the original logit.\n",
    "        top_k, top_p: top-k and top-p sampling parameters.\n",
    "            Note that top-k = 1 is gready. Also, these paramters are\n",
    "            exclusive meaning that:\n",
    "                if top-k > 0 then we expect top-p=0.\n",
    "                if top-p > 0 then we check for top-k=0.\n",
    "        temperature: sampling temperature.\n",
    "        use_eod_token_for_early_termination: if True, do early termination if\n",
    "            all the sequences have reached this token.\n",
    "        prevent_newline_after_colon: if True, it will disable generating new line \\n after :\n",
    "    Outputs: Note that is size is adjusted to a lower value than\n",
    "             max-sequence-length if generation is terminated early.\n",
    "        tokens: prompt and generated tokens. size: [b, :]\n",
    "        generated_sequence_lengths: total length (including prompt) of\n",
    "            the generated sequence. size: [b]\n",
    "        output_log_probs: log probability of the selected tokens. size: [b, s]\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = tokens.size(0)\n",
    "    min_prompt_length = lengths\n",
    "    max_sequence_length = tokens.size(1)\n",
    "\n",
    "    # if max_sequence_length > args.max_position_embeddings:\n",
    "    #     raise ValueError(\"Length of prompt + tokens_to_generate longer than allowed\")\n",
    "\n",
    "    forward_step = ForwardStep(model, batch_size, max_sequence_length)\n",
    "\n",
    "    # =============\n",
    "    # Run infernece\n",
    "    # =============\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prev_context_length = 0\n",
    "        for context_length in range(min_prompt_length, max_sequence_length):\n",
    "            # Pick the slice that we need to pass through the network.\n",
    "            tokens2use = tokens[:, prev_context_length:context_length]\n",
    "            # logits will be meanigful only in the last pipeline stage.\n",
    "            logits = forward_step(tokens2use)\n",
    "            # Sample.\n",
    "            last_token_logits = logits[:, -1, :]\n",
    "            new_sample = torch.argmax(last_token_logits)\n",
    "            ############# isn't tokens shorter than this? shouldn't do concat? can we actually allocate the total tokens tensor to memory, or unnecessary?\n",
    "            tokens[0, context_length] = new_sample\n",
    "            # Update the context length for the next token generation.\n",
    "            prev_context_length = context_length\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m     tokens, lengths, output_log_probs, logits \u001b[38;5;241m=\u001b[39m generate_tokens_probs_and_return_on_first_stage(\n\u001b[1;32m      5\u001b[0m         model, context_tokens_tensor, context_length_tensor,\n\u001b[1;32m      6\u001b[0m         return_output_log_probs\u001b[38;5;241m=\u001b[39mreturn_output_log_probs,\n\u001b[1;32m      7\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k_sampling,\n\u001b[1;32m      8\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p_sampling,\n\u001b[1;32m      9\u001b[0m         top_p_decay\u001b[38;5;241m=\u001b[39mtop_p_decay,\n\u001b[1;32m     10\u001b[0m         top_p_bound\u001b[38;5;241m=\u001b[39mtop_p_bound,\n\u001b[1;32m     11\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m     12\u001b[0m         use_eod_token_for_early_termination\u001b[38;5;241m=\u001b[39muse_eod_token_for_early_termination,\n\u001b[1;32m     13\u001b[0m         stop_on_double_eol\u001b[38;5;241m=\u001b[39mstop_on_double_eol,\n\u001b[1;32m     14\u001b[0m         stop_on_eol\u001b[38;5;241m=\u001b[39mstop_on_eol,\n\u001b[1;32m     15\u001b[0m         prevent_newline_after_colon\u001b[38;5;241m=\u001b[39mprevent_newline_after_colon)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# tokens, prompts_plus_generations, prompts_plus_generations_segments = \\\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     detokenize_generations(tokens, lengths, True)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m prompts_plus_generations \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdetokenize(tokens\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tokens, lengths, output_log_probs, logits = generate_tokens_probs_and_return_on_first_stage(\n",
    "        model, context_tokens_tensor, context_length_tensor,\n",
    "        return_output_log_probs=return_output_log_probs,\n",
    "        top_k=top_k_sampling,\n",
    "        top_p=top_p_sampling,\n",
    "        top_p_decay=top_p_decay,\n",
    "        top_p_bound=top_p_bound,\n",
    "        temperature=temperature,\n",
    "        use_eod_token_for_early_termination=use_eod_token_for_early_termination,\n",
    "        stop_on_double_eol=stop_on_double_eol,\n",
    "        stop_on_eol=stop_on_eol,\n",
    "        prevent_newline_after_colon=prevent_newline_after_colon)\n",
    "\n",
    "\n",
    "# tokens, prompts_plus_generations, prompts_plus_generations_segments = \\\n",
    "#     detokenize_generations(tokens, lengths, True)\n",
    "prompts_plus_generations = tokenizer.detokenize(tokens.transpose(0, 1)[0].cpu().numpy().tolist())\n",
    "print(prompts_plus_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main generation function taken from megatron/text_generation/generation.py\n",
    "\n",
    "from megatron.text_generation.sampling import sample\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_tokens_probs_and_return_on_first_stage(\n",
    "        model, tokens, lengths,\n",
    "        return_output_log_probs=False,\n",
    "        top_k=0, top_p=0.0, top_p_decay=0.0, top_p_bound=0.0,\n",
    "        temperature=1.0,\n",
    "        use_eod_token_for_early_termination=True,\n",
    "        stop_on_double_eol=False,\n",
    "        stop_on_eol=False,\n",
    "        prevent_newline_after_colon=True\n",
    "        ):\n",
    "    \"\"\"Main token generation function.\n",
    "    Arguments:\n",
    "        model: no interleaving is supported.\n",
    "        tokens: prompt tokens extended to be of size [b, max-sequence-length]\n",
    "        lengths: original prompt length, size: [b]\n",
    "        return_output_log_probs: flag to calculate the log probability of\n",
    "            the generated tokens. Note that the log probability is the one\n",
    "            from the original logit.\n",
    "        top_k, top_p: top-k and top-p sampling parameters.\n",
    "            Note that top-k = 1 is gready. Also, these paramters are\n",
    "            exclusive meaning that:\n",
    "                if top-k > 0 then we expect top-p=0.\n",
    "                if top-p > 0 then we check for top-k=0.\n",
    "        temperature: sampling temperature.\n",
    "        use_eod_token_for_early_termination: if True, do early termination if\n",
    "            all the sequences have reached this token.\n",
    "        prevent_newline_after_colon: if True, it will disable generating new line \\n after :\n",
    "    Outputs: Note that is size is adjusted to a lower value than\n",
    "             max-sequence-length if generation is terminated early.\n",
    "        tokens: prompt and generated tokens. size: [b, :]\n",
    "        generated_sequence_lengths: total length (including prompt) of\n",
    "            the generated sequence. size: [b]\n",
    "        output_log_probs: log probability of the selected tokens. size: [b, s]\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = tokens.size(0)\n",
    "    min_prompt_length = lengths\n",
    "    max_sequence_length = tokens.size(1)\n",
    "\n",
    "    # if max_sequence_length > args.max_position_embeddings:\n",
    "    #     raise ValueError(\"Length of prompt + tokens_to_generate longer than allowed\")\n",
    "\n",
    "    forward_step = ForwardStep(model, batch_size, max_sequence_length)\n",
    "\n",
    "    # if hasattr(args, 'eos_id'):\n",
    "    #     termination_id = args.eos_id\n",
    "    # else:\n",
    "    termination_id = tokenizer.eod\n",
    "\n",
    "    # ===================\n",
    "    # Pre-allocate memory\n",
    "    # ===================\n",
    "\n",
    "    # Log probability of the sequence (prompt + generated tokens).\n",
    "    output_log_probs = None\n",
    "    output_log_probs_size = (batch_size, max_sequence_length - 1)\n",
    "    # Lengths of generated seuquence including including prompts.\n",
    "    generated_sequence_lengths = None\n",
    "\n",
    "    if return_output_log_probs:\n",
    "        output_log_probs = torch.empty(output_log_probs_size,\n",
    "                                        dtype=torch.float32,\n",
    "                                        device=torch.cuda.current_device())\n",
    "    generated_sequence_lengths = torch.ones(\n",
    "            batch_size, dtype=torch.int64,\n",
    "            device=torch.cuda.current_device()) * max_sequence_length\n",
    "    \n",
    "    # Whether we have reached a termination id.\n",
    "    is_generation_done = torch.zeros(batch_size, dtype=torch.uint8,\n",
    "                                     device=torch.cuda.current_device())\n",
    "\n",
    "    # =============\n",
    "    # Run infernece\n",
    "    # =============\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prev_context_length = 0\n",
    "        if not lengths == min_prompt_length:\n",
    "            print(\"We want all prompt lengths to be the same, otherwise it seems from the algo below that prompts longer than `min_prompt_length` will be cut.\")\n",
    "        for context_length in range(min_prompt_length, max_sequence_length):\n",
    "\n",
    "            # Pick the slice that we need to pass through the network.\n",
    "            tokens2use = tokens[:, prev_context_length:context_length]\n",
    "\n",
    "            # logits will be meanigful only in the last pipeline stage.\n",
    "            logits = forward_step(tokens2use)\n",
    "\n",
    "            \n",
    "            if prevent_newline_after_colon:\n",
    "                logits[tokens2use[:, -1] == tokenizer.tokenize(':')[0], -1, tokenizer.tokenize('\\n')[0]] = -1e10 # disable \"\\n\" after \":\"\n",
    "            assert logits is not None\n",
    "\n",
    "            # Sample.\n",
    "            last_token_logits = logits[:, -1, :]\n",
    "            new_sample = sample(last_token_logits,\n",
    "                                top_k=top_k,\n",
    "                                top_p=top_p,\n",
    "                                temperature=temperature,\n",
    "                                vocab_size=tokenizer.vocab_size)\n",
    "            if top_p > 0.0 and top_p_decay > 0.0:\n",
    "                top_p = top_p * top_p_decay\n",
    "                if top_p_bound > 0.0:\n",
    "                    top_p = max(top_p, top_p_bound)\n",
    "\n",
    "            # If a prompt length is smaller or equal th current context\n",
    "            # length, it means we have started generating tokens\n",
    "            started = lengths <= context_length\n",
    "            # Update the tokens.\n",
    "            tokens[started, context_length] = new_sample[started]\n",
    "\n",
    "            # Calculate the log probabilities.\n",
    "            if return_output_log_probs:\n",
    "                log_probs = F.log_softmax(logits, dim=2)\n",
    "                if return_output_log_probs:\n",
    "                    # Pick the tokens that we need to get the log\n",
    "                    # probabilities for. Note that next input token is\n",
    "                    # the token which we selected in the current logits,\n",
    "                    # so shift by 1.\n",
    "                    indices = torch.unsqueeze(\n",
    "                        tokens[\n",
    "                            :,\n",
    "                            (prev_context_length + 1):(context_length + 1)],\n",
    "                        2)\n",
    "                    output_log_probs[:,\n",
    "                                        prev_context_length:context_length] = \\\n",
    "                        torch.gather(log_probs, 2, indices).squeeze(2)\n",
    "\n",
    "            # Update the context length for the next token generation.\n",
    "            prev_context_length = context_length\n",
    "\n",
    "            # Check if all the sequences have hit the termination_id.\n",
    "            done = None\n",
    "            \n",
    "            # TODO(rprenger) These stopping methods are tokenizer dependent\n",
    "            # instead tokenization should be in the inference loop so stop sequences can be used\n",
    "            if stop_on_double_eol:\n",
    "                hit_double_eol = (new_sample == 628).byte() & started.byte()\n",
    "                hit_two_eols = (new_sample == 198).byte() & (tokens[:, context_length-1] == 198).byte() & started.byte()\n",
    "                done_token = hit_double_eol | hit_two_eols\n",
    "            elif stop_on_eol:\n",
    "                hit_double_eol = (new_sample == 628).byte() & started.byte()\n",
    "                hit_eol = (new_sample == 198).byte() & started.byte()\n",
    "                done_token = hit_double_eol | hit_eol\n",
    "            else: \n",
    "                done_token = (new_sample == termination_id).byte() & \\\n",
    "                    started.byte()\n",
    "            \n",
    "            just_finished = (done_token & ~is_generation_done).bool()\n",
    "            generated_sequence_lengths[just_finished.view(-1)] = \\\n",
    "                context_length + 1\n",
    "            is_generation_done = is_generation_done | done_token\n",
    "            done = torch.all(is_generation_done)\n",
    "            if use_eod_token_for_early_termination and done:\n",
    "                break\n",
    "            \n",
    "\n",
    "    tokens = tokens[:, :(max_sequence_length + 1)]\n",
    "\n",
    "    if return_output_log_probs:\n",
    "        output_log_probs = output_log_probs[:, :context_length]\n",
    "\n",
    "    return tokens, generated_sequence_lengths, output_log_probs, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
